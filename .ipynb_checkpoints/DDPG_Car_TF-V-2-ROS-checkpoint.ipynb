{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import tensorflow as tf\n",
    "import json, sys, os\n",
    "from os import path\n",
    "import random\n",
    "from collections import deque\n",
    "from tqdm import tqdm,trange\n",
    "\n",
    "#env_to_use = 'Pendulum-v0'\n",
    "env_to_use = 'CarRacing-v0'\n",
    "\n",
    "\n",
    "# hyperparameters\n",
    "\n",
    "h1_actor = 8\n",
    "h2_actor = 8\n",
    "h3_actor = 8\n",
    "h1_critic = 8\n",
    "h2_critic = 8\n",
    "h3_critic = 8\n",
    "gamma = 0.99\n",
    "lr_actor = 1e-3\n",
    "lr_critic = 1e-3\n",
    "lr_decay = 1\n",
    "l2_reg_actor = 1e-6\n",
    "l2_reg_critic = 1e-6\n",
    "dropout_actor = 0\n",
    "dropout_critic = 0\n",
    "num_episodes = 100\n",
    "max_steps_ep = 3000\n",
    "tau = 1e-2\n",
    "train_every = 1\n",
    "replay_memory_capacity = int(1e5)\n",
    "minibatch_size = 64#1024\n",
    "initial_noise_scale = 0.1\n",
    "noise_decay = 0.99\n",
    "exploration_mu = 0.0\n",
    "exploration_theta = 0.15\n",
    "exploration_sigma = 0.2\n",
    "\n",
    "# game parameters\n",
    "env = gym.make(env_to_use)\n",
    "state_dim = env.observation_space.shape\n",
    "action_dim = np.prod(np.array(env.action_space.shape))\n",
    "\n",
    "# set seeds to 0\n",
    "env.seed(0)\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.high - env.action_space.low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_memory = deque(maxlen=replay_memory_capacity)\n",
    "\n",
    "def add_to_memory(experience):\n",
    "    replay_memory.append(experience)\n",
    "\n",
    "def sample_from_memory(minibatch_size):\n",
    "    return random.sample(replay_memory, minibatch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\irfan\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "#import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "class ANN():\n",
    "    tf.reset_default_graph()\n",
    "    state_ph  =  tf.placeholder(dtype=tf.float32, shape=[None,state_dim[0],state_dim[1],state_dim[2]])\n",
    "    action_ph = tf.placeholder(dtype=tf.float32, shape=[None,action_dim])\n",
    "    reward_ph = tf.placeholder(dtype=tf.float32, shape=[None])\n",
    "    next_state_ph = tf.placeholder(dtype=tf.float32, shape=[None,state_dim[0],state_dim[1],state_dim[2]])\n",
    "    is_not_terminal_ph = tf.placeholder(dtype=tf.float32, shape=[None])\n",
    "    \n",
    "\n",
    "    \n",
    "    episodes = tf.Variable(0.0, trainable=False, name='episodes')\n",
    "    episode_inc_op = episodes.assign_add(1)\n",
    "    \n",
    "    def __init__(self):\n",
    "        with tf.variable_scope('actor'):\n",
    "            self.actor_net_value = ANN.generate_actor_network(self,trainable = True, reuse = False)\n",
    "              \n",
    "        \n",
    "        with tf.variable_scope('slow_target_actor', reuse=False):\n",
    "            self.target_actor_net_value = tf.stop_gradient(ANN.generate_actor_network(self,trainable = False, reuse = False))\n",
    "\n",
    "        with tf.variable_scope('critic') as scope:\n",
    "            self.critic_net_value = ANN.generate_critic_network(self,trainable = True, reuse = False)\n",
    "            self.q_value_for_actor_net = ANN.generate_critic_network(self,trainable = True, reuse = True,mode=2)\n",
    "\n",
    "        \n",
    "        with tf.variable_scope('slow_target_critic', reuse=False):\n",
    "            self.target_critic_net_value = tf.stop_gradient(ANN.generate_critic_network(self,trainable = False, reuse = False,mode=3))\n",
    "        \n",
    "        \n",
    "        self.actor_net_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='actor')\n",
    "        self.target_actor_net_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='slow_target_actor')\n",
    "        self.critic_net_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='critic')\n",
    "        self.target_critic_net_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='slow_target_critic')\n",
    "\n",
    "        \n",
    "    def predict_graph(self):\n",
    "        return self.actor_net_value\n",
    "    def generate_actor_network(self,trainable, reuse):\n",
    "        layer1_out = tf.layers.conv2d(ANN.state_ph, filters=16, kernel_size=[8, 8],\n",
    "                                      strides=[4, 4], padding='same', activation=tf.nn.relu, data_format='channels_last', name='actor_layer1_out')\n",
    "        layer2_out = tf.layers.conv2d(layer1_out, filters=32, kernel_size=[4, 4],\n",
    "                                      strides=[2, 2], padding='same', activation=tf.nn.relu, data_format='channels_last', name='actor_layer2_out')\n",
    "        layer3_out = tf.layers.dense(tf.layers.flatten(layer2_out), 256, activation=tf.nn.relu, name='actor_layer3_out')\n",
    "        #output = tf.layers.dense(layer3_out, action_size, activation=None, name='output')\n",
    "        #hidden = tf.layers.dense(ANN.state_ph, h1_actor, activation = tf.nn.relu, trainable = trainable, name = 'dense', reuse = reuse)\n",
    "        #hidden_2 = tf.layers.dense(hidden, h2_actor, activation = tf.nn.relu, trainable = trainable, name = 'dense_1', reuse = reuse)\n",
    "        #hidden_3 = tf.layers.dense(hidden_2, h3_actor, activation = tf.nn.relu, trainable = trainable, name = 'dense_2', reuse = reuse)\n",
    "        actions_unscaled = tf.layers.dense(layer3_out, action_dim, trainable = trainable, name = 'dense_3', reuse = reuse)\n",
    "        actions = env.action_space.low + tf.nn.sigmoid(actions_unscaled)*(env.action_space.high - env.action_space.low) # bound the actions to the valid range\n",
    "        return actions_unscaled\n",
    "\n",
    "   \n",
    "    \n",
    "    def generate_critic_network(self,trainable, reuse,mode=1):\n",
    "        layer1_out = tf.layers.conv2d(ANN.state_ph, filters=16, kernel_size=[8, 8],\n",
    "                                      strides=[4, 4], padding='same', activation=tf.nn.relu, data_format='channels_last', name='critic_layer1_out',reuse = reuse)\n",
    "        layer2_out = tf.layers.conv2d(layer1_out, filters=32, kernel_size=[4, 4],\n",
    "                                      strides=[2, 2], padding='same', activation=tf.nn.relu, data_format='channels_last', name='critic_layer2_out',reuse = reuse)\n",
    "        layer2_flat=tf.layers.flatten(layer2_out)\n",
    "        if mode==1:\n",
    "            state_action = tf.concat([layer2_flat, ANN.action_ph], axis=1)\n",
    "        if mode==2:\n",
    "            state_action = tf.concat([layer2_flat,self.actor_net_value], axis=1)\n",
    "        if mode==3:\n",
    "            state_action = tf.concat([layer2_flat,self.target_actor_net_value], axis=1)\n",
    "        layer3_out = tf.layers.dense(state_action, 256, activation=tf.nn.relu, name='critic_layer3_out',reuse = reuse)\n",
    "        #hidden = tf.layers.dense(state_action, h1_critic, activation = tf.nn.relu, trainable = trainable, name = 'dense', reuse = reuse)\n",
    "        #hidden_2 = tf.layers.dense(hidden, h2_critic, activation = tf.nn.relu, trainable = trainable, name = 'dense_1', reuse = reuse)\n",
    "        #hidden_3 = tf.layers.dense(hidden_2, h3_critic, activation = tf.nn.relu, trainable = trainable, name = 'dense_2', reuse = reuse)\n",
    "        q_values = tf.layers.dense(layer3_out, 1, trainable = trainable, name = 'dense_3', reuse = reuse)\n",
    "        return q_values\n",
    "    def train_graph(self):\n",
    "        updated_q_values = tf.expand_dims(ANN.reward_ph, 1) + tf.expand_dims(ANN.is_not_terminal_ph, 1) * gamma * self.target_critic_net_value\n",
    "        td_errors = updated_q_values - self.critic_net_value\n",
    "\n",
    "\n",
    "        critic_loss = tf.reduce_mean(tf.square(td_errors))\n",
    "        for var in self.critic_net_vars:\n",
    "            if not 'bias' in var.name:\n",
    "                critic_loss += l2_reg_critic * 0.5 * tf.nn.l2_loss(var)\n",
    "\n",
    "\n",
    "        critic_train_op = tf.train.AdamOptimizer(lr_critic).minimize(critic_loss)\n",
    "        actor_loss = -1*tf.reduce_mean(self.q_value_for_actor_net)\n",
    "        for var in self.actor_net_vars:\n",
    "            if not 'bias' in var.name:\n",
    "                actor_loss += l2_reg_actor * 0.5 * tf.nn.l2_loss(var)\n",
    "\n",
    "        actor_train_op = tf.train.AdamOptimizer(lr_actor).minimize(actor_loss, var_list=self.actor_net_vars)\n",
    "        return actor_train_op,critic_train_op\n",
    "        \n",
    "    def update_wts_graph(self):\n",
    "            update_slow_target_ops = []\n",
    "            for i, target_actor_var in enumerate(self.target_actor_net_vars):\n",
    "                update_slow_target_actor_op = target_actor_var.assign(tau*self.actor_net_vars[i]+(1-tau)*target_actor_var)\n",
    "                update_slow_target_ops.append(update_slow_target_actor_op)\n",
    "\n",
    "            for i, slow_target_var in enumerate(self.target_critic_net_vars):\n",
    "                update_slow_target_critic_op = slow_target_var.assign(tau*self.critic_net_vars[i]+(1-tau)*slow_target_var)\n",
    "                update_slow_target_ops.append(update_slow_target_critic_op)\n",
    "\n",
    "            update_slow_targets_op = tf.group(*update_slow_target_ops, name='update_slow_targets')\n",
    "            return update_slow_targets_op \n",
    "    def load_weights(self,_actor_net_vars,_critic_net_vars):\n",
    "            update_wts_ops = []\n",
    "            for i, actor_var in enumerate(self.actor_net_vars):\n",
    "                update_actor_op = actor_var.assign(_actor_net_vars[i])\n",
    "                update_wts_ops.append(update_actor_op)\n",
    "\n",
    "            for i, critic_var in enumerate(self.critic_net_vars):\n",
    "                update_critic_op = critic_var.assign(_critic_net_vars[i])\n",
    "                update_wts_ops.append(update_critic_op)\n",
    "\n",
    "            update_wts_ops = tf.group(*update_wts_ops, name='update_wts_ops')\n",
    "            return update_wts_ops \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-d2ec2c24b181>:46: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "WARNING:tensorflow:From C:\\Users\\irfan\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_core\\python\\layers\\convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-4-d2ec2c24b181>:49: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-4-d2ec2c24b181>:49: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n"
     ]
    }
   ],
   "source": [
    "Model=ANN()\n",
    "actor_net_value=Model.predict_graph()\n",
    "actor_train_op,critic_train_op=Model.train_graph()\n",
    "update_wts_op=Model.update_wts_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# initialize session\n",
    "#sess = tf.Session()\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1143..1442 -> 299-tiles track\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-e157315f8879>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m                     \u001b[0mModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_ph\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m                     \u001b[0mModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreward_ph\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m                     \u001b[0mModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_state_ph\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m                     \u001b[0mModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_not_terminal_ph\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m                     })\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#####################################################################################################\n",
    "## Training\n",
    "\n",
    "total_steps = 0\n",
    "log_rewards=[]\n",
    "for ep in range(num_episodes):\n",
    "\n",
    "    total_reward = 0\n",
    "    steps_in_ep = 0\n",
    "\n",
    "    \n",
    "    noise_process = np.zeros(action_dim)\n",
    "    noise_scale = (initial_noise_scale * noise_decay**ep) * (env.action_space.high - env.action_space.low)\n",
    "\n",
    "    \n",
    "    observation = env.reset()\n",
    "    for t in range(max_steps_ep):\n",
    "        \n",
    "        action_for_state, = sess.run(actor_net_value, \n",
    "            feed_dict = {Model.state_ph: observation[None]/255.0})\n",
    "\n",
    "        \n",
    "        noise_process = exploration_theta*(exploration_mu - noise_process) + exploration_sigma*np.random.randn(action_dim)\n",
    "        action_for_state += noise_scale*noise_process\n",
    "\n",
    "       \n",
    "        next_observation, reward, done, _info = env.step(action_for_state)\n",
    "        total_reward += reward\n",
    "\n",
    "        add_to_memory((observation, action_for_state, reward, next_observation, \n",
    "            0.0 if done else 1.0))\n",
    "        \n",
    "        if total_steps%train_every == 0 and len(replay_memory) >= minibatch_size:\n",
    "\n",
    "           \n",
    "            minibatch = sample_from_memory(minibatch_size)\n",
    "\n",
    "            _, _ = sess.run([critic_train_op, actor_train_op], \n",
    "                feed_dict = {\n",
    "                    Model.state_ph: np.asarray([elem[0] for elem in minibatch]),\n",
    "                    Model.action_ph: np.asarray([elem[1] for elem in minibatch]),\n",
    "                    Model.reward_ph: np.asarray([elem[2] for elem in minibatch]),\n",
    "                    Model.next_state_ph: np.asarray([elem[3] for elem in minibatch]),\n",
    "                    Model.is_not_terminal_ph: np.asarray([elem[4] for elem in minibatch]),\n",
    "                    })\n",
    "\n",
    "\n",
    "            _ = sess.run(update_wts_op)\n",
    "\n",
    "        observation = next_observation\n",
    "        total_steps += 1\n",
    "        steps_in_ep += 1\n",
    "        \n",
    "        if done: \n",
    "            \n",
    "            _ = sess.run(Model.episode_inc_op)\n",
    "            break\n",
    "    log_rewards.append([ep,total_reward])\n",
    "    print('Episode %2i, Reward: %7.3f, Steps: %i, noise: %7.3f'%(ep,total_reward,steps_in_ep, noise_scale[0]))\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c conda-forge tqdm\n",
    "#conda install -c conda-forge ipywidgets\n",
    "#conda install -c conda-forge nodejs#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tqdm.write('Episode %2i, Reward: %7.3f, Steps: %i, noise: %7.3f'%(ep,total_reward,steps_in_ep, noise_scale[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_weights(actor_net_var,critic_net_vars):\n",
    "            import pickle\n",
    "            f=open('net_wts','wb')\n",
    "            pickle.dump([actor_wts,critic_wts],f)\n",
    "            f.close() \n",
    "def load_weights(Model,sess):\n",
    "            import pickle\n",
    "            f=open('net_wts','rb')\n",
    "            [actor_wts,critic_wts]=pickle.load(f)\n",
    "            f.close()\n",
    "            sess.run(Model.load_weights(actor_wts,critic_wts))\n",
    "            return actor_wts,critic_wts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save=True\n",
    "if save==True:\n",
    "    actor_wts=sess.run(Model.actor_net_vars)\n",
    "    critic_wts=sess.run(Model.critic_net_vars)\n",
    "    save_weights(actor_wts,critic_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "load=True\n",
    "if load==True:\n",
    "    load_weights(Model,sess)\n",
    "    a_wts=sess.run(Model.actor_net_vars)\n",
    "    c_wts=sess.run(Model.critic_net_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1143..1442 -> 299-tiles track\n",
      "[0.99999976 1.         0.999992  ]\n",
      "[1.        1.        0.9999999]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import cv2\n",
    "env=gym.make(env_to_use)\n",
    "env.seed(0)\n",
    "obs=env.reset()\n",
    "\n",
    "#os.mkdir(env_to_use+'Test')\n",
    "#os.mkdir(env_to_use+'Test/img/')\n",
    "for i in range(1000):\n",
    "    img=obs[None]/255.0\n",
    "    _action, = sess.run(actor_net_value, \n",
    "                    feed_dict = {Model.state_ph: img})\n",
    "    print(_action)\n",
    "    obs,rew,done,info=env.step(_action)\n",
    "    img=env.render(mode='rgb_array')\n",
    "    time.sleep(0.1)\n",
    "    #cv2.imwrite(env_to_use+'Test/img/'+str(i)+'.jpg',img)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "rws=np.array(log_rewards)\n",
    "fig=plt.figure(figsize=(10,10))\n",
    "plt.plot(rws[:,0],rws[:,1])\n",
    "plt.title('epoch vs sum of reward')\n",
    "plt.savefig(env_to_use+'Test/'+'rewards.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame(rws[:,1],columns=['sum_of_rewards'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(env_to_use+'Test/rewards.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
