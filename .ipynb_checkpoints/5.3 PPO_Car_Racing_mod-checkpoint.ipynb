{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install tensorflow-gpu\n",
    "#pip install gym\n",
    "#pip install box2d\n",
    "#pip install pandas\n",
    "#pip install matplotlib\n",
    "#pip install opencv-python\n",
    "#pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1\n",
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "os.environ['PYTHONHASHSEED']=str(seed)\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "import tensorflow\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense ,Input,concatenate ,Conv2D,Convolution2D,Conv2DTranspose,\\\n",
    "MaxPooling2D,AveragePooling2D, LSTM ,Reshape, TimeDistributed,ReLU, LeakyReLU, Dropout, BatchNormalization, Flatten, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam,Adagrad\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import backend as K\n",
    "import warnings\n",
    "import glob\n",
    "import math\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import deque\n",
    "from tqdm import tqdm,trange\n",
    "import gym\n",
    "from copy import copy , deepcopy\n",
    "import cProfile\n",
    "#from tensorflow.keras.callbacks import TensorBoard\n",
    "#from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "TanH=tensorflow.keras.layers.Activation('tanh')\n",
    "Sigmoid=tensorflow.keras.layers.Activation('sigmoid')\n",
    "#tf.test.is_gpu_available()\n",
    "#tf.keras.backend.set_floatx('float64')\n",
    "physical_devices=tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_steering_speed_gyro_abs(_img):\n",
    "    right_steering = _img[6, 36:46].mean()/255\n",
    "    left_steering = _img[6, 26:36].mean()/255\n",
    "    _steering = (right_steering - left_steering + 1.0)/2\n",
    "    \n",
    "    l_gyro = _img[6, 46:60].mean()/255\n",
    "    r_gyro = _img[6, 60:76].mean()/255\n",
    "    _gyro = (r_gyro - l_gyro + 1.0)/2\n",
    "    \n",
    "    \n",
    "    _speed =_img[:, 0][:-2].mean()/255\n",
    "    _abs1 = _img[:, 6][:-2].mean()/255\n",
    "    _abs2 = _img[:, 8][:-2].mean()/255\n",
    "    _abs3 = _img[:, 10][:-2].mean()/255\n",
    "    _abs4 = _img[:, 12][:-2].mean()/255\n",
    "    \n",
    "    return [_steering, _speed, _gyro, _abs1, _abs2, _abs3, _abs4]\n",
    "def get_states(img):\n",
    "    img=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY).reshape((img.shape[0],img.shape[1],1))\n",
    "    sensor_values=compute_steering_speed_gyro_abs(img)\n",
    "    img=np.array(img[:84,6:-6,:]/255.0,dtype=np.float32)\n",
    "    return img,sensor_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "act=Actor(3)\n",
    "act.build(input_shape=[[1,96,96,1],[1,6]])\n",
    "act.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('Unrecognized keyword arguments:', dict_keys(['input_shape']))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-4f18aa5c9ff5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[0mn_act\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m \u001b[0m_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mppo_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m \u001b[0mppo_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[0mtrain_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-4f18aa5c9ff5>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mppo_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_actor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mActor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_act\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_critic\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCritic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_debug\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-4f18aa5c9ff5>\u001b[0m in \u001b[0;36mActor\u001b[1;34m(n_act)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mActor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_act\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mstate_1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m84\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m84\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mstate_2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\input_layer.py\u001b[0m in \u001b[0;36mInput\u001b[1;34m(shape, batch_size, name, dtype, sparse, tensor, ragged, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unrecognized keyword arguments:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtensor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: ('Unrecognized keyword arguments:', dict_keys(['input_shape']))"
     ]
    }
   ],
   "source": [
    "\n",
    "def Actor(n_act):\n",
    "    state_1=Input(input_shape=[84,84,1])\n",
    "    state_2=Input(input_shape=[7])\n",
    "    x = Conv2D(16,strides=4,kernel_size=7,activation='relu')(state_1)\n",
    "    x = Conv2D(32,strides=2,kernel_size=3,activation='relu')(x)\n",
    "    x = Conv2D(64,strides=2,kernel_size=3,activation='relu')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Concatenate()([state_1,state_2])\n",
    "    x = Dense(512, activation='relu')\n",
    "    x = Dense(256, activation='relu')\n",
    "    mean = Dense(n_act)\n",
    "    mean  = TanH(mean)\n",
    "    std = Dense(n_act)\n",
    "    std  = Sigmoid(std)#+10e-10\n",
    "    actor_model=Model(inputs=[state_1,state_2],outputs=[mean,std])\n",
    "    return actor_model\n",
    "    \n",
    "def Critic():\n",
    "    state_1=Input(input_shape=[84,84,1])\n",
    "    state_2=Input(input_shape=[7])\n",
    "    x = Conv2D(16,strides=4,kernel_size=7,activation='relu')(state_1)\n",
    "    x = Conv2D(32,strides=2,kernel_size=3,activation='relu')(x)\n",
    "    x = Conv2D(64,strides=2,kernel_size=3,activation='relu')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Concatenate()([state_1,state_2])\n",
    "    x = Dense(512, activation='relu')\n",
    "    x = Dense(256, activation='relu')\n",
    "    value = Dense(1)\n",
    "    critic_model=Model(inputs=[state_1,state_2],outputs=[value])\n",
    "    return critic_model\n",
    "\n",
    "class ppo_model():\n",
    "    def __init__(self):\n",
    "        self._actor=Actor(n_act)\n",
    "        self._critic=Critic()\n",
    "        self._debug = False\n",
    "        self._actor_opt= Adam(0.001)\n",
    "        self._critic_opt= Adam(0.001)\n",
    "        self.value_coef=0.5\n",
    "        self.entropy_coef=0.01\n",
    "        self._gamma=0.99\n",
    "        self._lambda=0.95\n",
    "        self.clip_epsilon=0.05\n",
    "        self._advantage=16\n",
    "        \n",
    "    def get_advantages(self,values, masks, rewards):\n",
    "        target_qvals = []\n",
    "        for i in range(len(rewards)-self._advantage):\n",
    "            _values=values[i:i+self._advantage+1]\n",
    "            _rewards=rewards[i:i+self._advantage]\n",
    "            _masks=masks[i:i+self._advantage]\n",
    "            gae = 0\n",
    "            for j in reversed(range(self._advantage)):\n",
    "                delta = _rewards[j] + self._gamma * _values[j + 1] * _masks[j] - _values[j]\n",
    "                gae = delta + self._gamma * self._lambda * _masks[j] * gae\n",
    "            target_qvals.append(gae + _values[0])\n",
    "        target_qvals = np.array(target_qvals)\n",
    "        adv = np.array(target_qvals) - values[:-self._advantage-1]\n",
    "        return target_qvals, (adv - np.mean(adv)) / (np.std(adv) + 1e-10)\n",
    "    \n",
    "    def _get_advantages(self,values, masks, rewards):\n",
    "        target_qvals = []\n",
    "        gae=0\n",
    "        for i in reversed(range(len(rewards))):\n",
    "                delta = rewards[j] + self.gamma * values[j + 1] * masks[j] - values[j]\n",
    "                gae = delta + self._gamma * self._lambda * masks[j] * gae\n",
    "        target_qvals.append(gae + _values[0])\n",
    "        target_qvals = np.array(target_qvals)\n",
    "        adv = np.array(target_qvals) - values[:-self._advantage-1]\n",
    "        return target_qvals, (adv - np.mean(adv)) / (np.std(adv) + 1e-10)\n",
    "    \n",
    "    def actor_loss(self,mean,std,old_mean,old_std,old_action,advantage_raw):#, rewards, values):\n",
    "            old_probs = self.get_log_probs(old_action,old_mean,old_std)\n",
    "            new_probs = self.get_log_probs(old_action,mean,std)\n",
    "            entropy_loss = self.get_entropy(std)\n",
    "            ratio = K.exp(new_probs-old_probs)\n",
    "            p1 = ratio * advantage_raw\n",
    "            p2 = K.clip(ratio, min_value=1 - self.clip_epsilon, max_value=1 + self.clip_epsilon) * advantage_raw\n",
    "            actor_loss = K.mean(K.minimum(p1, p2))\n",
    "            #critic_loss = 0#K.mean(K.square(rewards - values))\n",
    "            total_loss =   -actor_loss - self.entropy_coef * entropy_loss #* K.mean(-(new_probs * K.log(new_probs + 1e-10)))\n",
    "            #tf.print('A',tf.reduce_sum(new_probs),tf.reduce_sum(old_probs),'B',tf.reduce_sum(ratio))\n",
    "            return total_loss\n",
    "\n",
    "    @tf.function\n",
    "    def train_actor(self,curr_states,_old_mean,_old_std,_old_actions,_advantage_raw):\n",
    "             _advantage_raw=tf.cast(_advantage_raw,tf.float32)\n",
    "             with tf.GradientTape() as tape:\n",
    "                  mean,std = self._actor(curr_states, training=True)\n",
    "                  if self._debug ==True :tf.print('1 : train_actor :',tf.reduce_sum(curr_states),tf.reduce_sum(mean),tf.reduce_sum(std))\n",
    "                  _act_loss = self.actor_loss(mean,std,_old_mean,_old_std,_old_actions,_advantage_raw)\n",
    "                  gradients = tape.gradient(_act_loss, self._actor.trainable_variables)\n",
    "                  self._actor_opt.apply_gradients(zip(gradients, self._actor.trainable_variables))\n",
    "                  #tf.print('actor grad : ',gradients[0])\n",
    "             return  _act_loss\n",
    "\n",
    "    @tf.function          \n",
    "    def critic_loss(self,_values,_target_rewards):\n",
    "            batch_size=_values.shape[0]\n",
    "            critic_loss=self.value_coef*2*tf.reduce_sum(tf.square(_target_rewards-_values))/batch_size\n",
    "            return critic_loss\n",
    "        \n",
    "    @tf.function\n",
    "    def train_critic(self,_states,_target_rewards):\n",
    "            with tf.GradientTape() as tape:\n",
    "                _values=self._critic(_states)\n",
    "                _values=tf.cast(_values,tf.float32)\n",
    "                _target_rewards=tf.cast(_target_rewards,tf.float32)\n",
    "                _critic_loss=self.critic_loss(_values,_target_rewards)\n",
    "                gradients = tape.gradient(_critic_loss, self._critic.trainable_variables)\n",
    "                self._critic_opt.apply_gradients(zip(gradients, self._critic.trainable_variables))\n",
    "                #tf.print('critic grad : ',gradients[0])\n",
    "            return _critic_loss\n",
    "    @tf.function\n",
    "    def get_log_probs(self,_actions,mean,std):\n",
    "        _actions=tf.cast(_actions,tf.float32)\n",
    "        mean=tf.cast(mean,tf.float32)\n",
    "        std=tf.cast(std,tf.float32)\n",
    "        _var = -0.5*((_actions-mean)/(std))**2\n",
    "        _coef = 1/(std*tf.sqrt(2*np.pi)) \n",
    "        \n",
    "        _probs = _coef*tf.cast(tf.exp(_var),tf.float32)\n",
    "        #_probs = tf.abs(_probs+ )\n",
    "        log_probs = tf.math.log(_probs)\n",
    "        #if _debug==True:\n",
    "        #    tf.print('2 : log_prob : _var  ',tf.reduce_sum(_var),tf.reduce_sum(_coef),tf.reduce_sum(_actions))\n",
    "        #    tf.print('2 : log_prob : _probs',tf.reduce_sum(_probs),tf.reduce_sum(log_probs))\n",
    "        return log_probs\n",
    "    def get_entropy(self,std):\n",
    "        entropy = 0.5 * (tf.math.log(2 * np.pi * std ** 2) + 1)\n",
    "        return entropy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_reward():\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        state = get_states(state)\n",
    "        st_1=K.expand_dims(state[0], 0)\n",
    "        st_2=K.expand_dims(state[1], 0)\n",
    "        mean,std = _model._actor([st_1,st_2])\n",
    "        act=np.random.normal(mean,std)[0]\n",
    "        next_state, reward, done, _ = env.step(np.clip(mean[0],-1,1))\n",
    "        #if reward == -100:\n",
    "        #    reward=-2\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "    return total_reward\n",
    "    \n",
    "image_based = False\n",
    "\n",
    "env=gym.make('CarRacing-v0')\n",
    "env.seed(seed)\n",
    "\n",
    "\n",
    "n_state = env.observation_space.shape[0]\n",
    "n_act = env.action_space.shape[0]\n",
    "\n",
    "_model=ppo_model()\n",
    "ppo_steps = 256\n",
    "train_epochs = 8\n",
    "batch_size = 32\n",
    "target_reached = False\n",
    "best_reward = 0\n",
    "iters = 0\n",
    "episodes = 1000\n",
    "adv=_model._advantage\n",
    "rewards_log=[]\n",
    "max_reward=0\n",
    "\n",
    "if not os.path.exists('replay_buffer_ppo'):\n",
    "       os.mkdir('replay_buffer_ppo') \n",
    "for episode in range(episodes):\n",
    "\n",
    "    \n",
    "    old_actions = []\n",
    "    old_probs =[]\n",
    "    values = []\n",
    "    masks = []\n",
    "    rewards = []\n",
    "    old_means = []\n",
    "    old_stds  = []\n",
    "    state_input = None\n",
    "    sum_reward=0\n",
    "    sum_reward_log=0\n",
    "    state = env.reset()\n",
    "    state=get_states(state)\n",
    "    for itr in trange(ppo_steps+adv):\n",
    "        st_1 = K.expand_dims(state[0], 0)\n",
    "        st_2 = K.expand_dims(state[1], 0)\n",
    "        mean,std = _model._actor([st_1,st_2])\n",
    "        q_value  = _model._critic([st_1,st_2])\n",
    "        action = np.random.normal(mean,std)[0]\n",
    "        #prob=get_log_probs(action,mean,std)\n",
    "        observation, reward, done, info = env.step(np.clip(action,-1,1))\n",
    "        state=get_states(observation)\n",
    "        #if reward == -100:\n",
    "        #    reward=-2\n",
    "        #print('itr: ' + str(itr) + ', action=' + str(action) + ', reward=' + str(reward) + ', q val=' + str(q_value.numpy()))\n",
    "        mask = not done\n",
    "        with open('replay_buffer_ppo/'+str(itr)+'.pkl','wb') as file:\n",
    "                      pickle.dump(state,file)\n",
    "        #_curr_states_1.append(state[0])\n",
    "        #_curr_states_2.append(state[1])\n",
    "        old_actions.append(action)\n",
    "        #old_probs.append(prob)\n",
    "        old_means.append(mean)\n",
    "        old_stds.append(std)\n",
    "        values.append(q_value)\n",
    "        masks.append(mask)\n",
    "        rewards.append(reward)\n",
    "        sum_reward+=reward\n",
    "\n",
    "        state = get_states(observation)\n",
    "        if done:\n",
    "            sum_reward_log=copy(sum_reward)\n",
    "            sum_reward=0\n",
    "            state=env.reset()\n",
    "            state = get_states(state)\n",
    "        \n",
    "    st_1=K.expand_dims(state[0], 0)\n",
    "    st_2=K.expand_dims(state[1], 0)\n",
    "    q_value = _model._critic([st_1,st_2])\n",
    "    values.append(q_value)\n",
    "    #_curr_states_1=np.array(_curr_states_1)#.reshape(-1,n_state)\n",
    "    #_curr_states_2=np.array(_curr_states_2)#.reshape(-1,n_state)\n",
    "    _old_actions=np.array(old_actions).reshape(-1,n_act)\n",
    "    _old_means=np.array(old_means).reshape(-1,n_act)\n",
    "    _old_stds=np.array(old_stds).reshape(-1,n_act)\n",
    "    _values=np.array(values).reshape(-1,1)\n",
    "    _rewards=np.array(rewards).reshape(-1,1)\n",
    "    _masks  = np.array(masks).reshape(-1,1)\n",
    "    \n",
    "    _target_qvals, _advantages = _model.get_advantages(_values, _masks, _rewards)\n",
    "    for _ in range(train_epochs):\n",
    "        no_of_batch=(ppo_steps//batch_size)\n",
    "        index=np.arange(ppo_steps)\n",
    "        np.random.shuffle(index)\n",
    "        for batch in range(no_of_batch):\n",
    "            _ind=index[batch*batch_size:(batch+1)*batch_size]\n",
    "            \n",
    "            _curr_states_1 = []\n",
    "            _curr_states_2 = []\n",
    "            for __ind in _ind:\n",
    "                with open('replay_buffer_ppo/'+str(__ind)+'.pkl','rb') as file:\n",
    "                    state= pickle.load(file)\n",
    "                    _curr_states_1.append(state[0])\n",
    "                    _curr_states_2.append(state[1])\n",
    "            _curr_states_1=np.array(_curr_states_1)\n",
    "            _curr_states_2=np.array(_curr_states_2)\n",
    "            \n",
    "            _model.train_actor([_curr_states_1,_curr_states_2],_old_means[_ind],_old_stds[_ind],_old_actions[_ind],_advantages[_ind])\n",
    "            _model.train_critic([_curr_states_1,_curr_states_2],_target_qvals[_ind])                            \n",
    "    #del _curr_states_1,_curr_states_2\n",
    "    avg_reward=0\n",
    "    for _ in range(5):\n",
    "        avg_reward += test_reward()\n",
    "    avg_reward/=5\n",
    "    print('episode :',episode,'reward :',sum_reward_log)   \n",
    "    print('total test reward=' + str(avg_reward))\n",
    "    if avg_reward >=max_reward:\n",
    "        _model._actor.save_weights('ppo_best_actor_lunar.hdf5')\n",
    "        _model._critic.save_weights('ppo_best_critic_lunar'+'.hdf5')\n",
    "        best_eps=episode\n",
    "        max_reward = avg_reward\n",
    "    rewards_log.append([sum_reward_log,avg_reward])\n",
    "    iters += 1\n",
    "    \n",
    "\n",
    "env.close()\n",
    "plt.plot(rewards_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_reward():\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        state = get_states(state)\n",
    "        st_1=K.expand_dims(state[0], 0)\n",
    "        st_2=K.expand_dims(state[1], 0)\n",
    "        mean,std = _model._actor([st_1,st_2])\n",
    "        act=np.random.normal(mean,std)[0]\n",
    "        print(mean,std,act)\n",
    "        next_state, reward, done, _ = env.step(mean[0])\n",
    "        if reward == -100:\n",
    "            reward=-2\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "    return total_reward\n",
    "test_reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_model._actor.save_weights('ppo-1000.hdf5')\n",
    "import pickle\n",
    "f=open('ppo-car-loss.pkl','wb')\n",
    "pickle.dump(rewards_log,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_model._actor.load_weights('ppo-1000.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(old_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(_old_means)\n",
    "plt.show()\n",
    "plt.plot(_old_stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rewards_log)\n",
    "plt.savefig('car-ppo.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.mkdir('car_ppo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TanH=tensorflow.keras.layers.Activation('tanh')\n",
    "Sigmoid=tensorflow.keras.layers.Activation('sigmoid')\n",
    "\n",
    "class Actor(Model):\n",
    "  def __init__(self,n_act):\n",
    "    super(Actor, self).__init__()\n",
    "    self.d1 = Dense(512, activation='relu')\n",
    "    self.d2 = Dense(256, activation='relu')\n",
    "    self.mean = Dense(n_act)\n",
    "    self.tanh  = TanH\n",
    "    self.sigmoid  = Sigmoid\n",
    "    self.std = Dense(n_act)\n",
    "\n",
    "  def call(self, x):\n",
    "    \n",
    "    x = self.d1(x)\n",
    "    x = self.d2(x)\n",
    "    #x = self.d3(x)\n",
    "    #x = self.d4(x)\n",
    "    mean = self.mean(x)\n",
    "    mean = self.tanh(mean)\n",
    "    std = self.std(x)\n",
    "    std = self.sigmoid(std)\n",
    "    return mean,std+10e-10\n",
    "\n",
    "def test_reward():\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        state_input = K.expand_dims(state, 0)\n",
    "        mean,std = _model._actor(state_input)\n",
    "        act=np.random.normal(mean,std)[0]\n",
    "        next_state, reward, done, _ = env.step(mean[0])\n",
    "        if reward == -100:\n",
    "            reward=-2\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "    return total_reward\n",
    "\n",
    "env = gym.make('CarRacing-v0')\n",
    "env.seed(seed)\n",
    "n_act=env.action_space.shape[0]\n",
    "n_state=env.observation_space.shape[0]\n",
    "state=env.reset()\n",
    "_actor=Actor(n_act)\n",
    "state=get_states(state)\n",
    "st_1 = K.expand_dims(state[0], 0)\n",
    "st_2 = K.expand_dims(state[1], 0)\n",
    "_actor([st_1,st_2])\n",
    "_actor.load_weights('ppo_best_actor_car.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state=env.reset()\n",
    "state=get_states(state)\n",
    "i=0\n",
    "ter_count=0\n",
    "while True:\n",
    "    st_1 = K.expand_dims(state[0], 0)\n",
    "    st_2 = K.expand_dims(state[1], 0)\n",
    "    action=_actor([st_1,st_2])\n",
    "    state,rwd,ter,info=env.step(action[0][0])\n",
    "    state=get_states(state)\n",
    "    img=env.render('rgb_array')\n",
    "    cv2.imwrite('car_ppo/'+str(i)+'.jpg',img)\n",
    "    i+=1\n",
    "    if ter==True:\n",
    "            ter_count+=1\n",
    "            state=env.reset()\n",
    "    if ter_count==5:\n",
    "             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
