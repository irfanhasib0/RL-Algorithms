{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import tensorflow as tf\n",
    "import json, sys, os\n",
    "from os import path\n",
    "import random\n",
    "from collections import deque\n",
    "from tqdm import tqdm,trange\n",
    "\n",
    "#env_to_use = 'Pendulum-v0'\n",
    "env_to_use = 'CarRacing-v0'\n",
    "\n",
    "\n",
    "# hyperparameters\n",
    "\n",
    "h1_actor = 8\n",
    "h2_actor = 8\n",
    "h3_actor = 8\n",
    "h1_critic = 8\n",
    "h2_critic = 8\n",
    "h3_critic = 8\n",
    "gamma = 0.99\n",
    "lr_actor = 1e-3\n",
    "lr_critic = 1e-3\n",
    "lr_decay = 1\n",
    "l2_reg_actor = 1e-6\n",
    "l2_reg_critic = 1e-6\n",
    "dropout_actor = 0\n",
    "dropout_critic = 0\n",
    "num_episodes = 100\n",
    "max_steps_ep = 3000\n",
    "tau = 1e-2\n",
    "train_every = 1\n",
    "replay_memory_capacity = int(1e5)\n",
    "minibatch_size = 16#1024\n",
    "initial_noise_scale = 0.1\n",
    "noise_decay = 0.99\n",
    "exploration_mu = 0.0\n",
    "exploration_theta = 0.15\n",
    "exploration_sigma = 0.2\n",
    "\n",
    "# game parameters\n",
    "env = gym.make(env_to_use)\n",
    "state_dim = env.observation_space.shape\n",
    "action_dim = np.prod(np.array(env.action_space.shape))\n",
    "\n",
    "# set seeds to 0\n",
    "env.seed(0)\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([2., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.high - env.action_space.low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_memory = deque(maxlen=replay_memory_capacity)\n",
    "\n",
    "def add_to_memory(experience):\n",
    "    replay_memory.append(experience)\n",
    "\n",
    "def sample_from_memory(minibatch_size):\n",
    "    return random.sample(replay_memory, minibatch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\irfan\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "WARNING:tensorflow:From C:\\Users\\irfan\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "#import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "class ANN():\n",
    "    tf.reset_default_graph()\n",
    "    state_ph  =  tf.placeholder(dtype=tf.float32, shape=[None,state_dim[0],state_dim[1],state_dim[2]])\n",
    "    action_ph = tf.placeholder(dtype=tf.float32, shape=[None,action_dim])\n",
    "    reward_ph = tf.placeholder(dtype=tf.float32, shape=[None])\n",
    "    next_state_ph = tf.placeholder(dtype=tf.float32, shape=[None,state_dim[0],state_dim[1],state_dim[2]])\n",
    "    is_not_terminal_ph = tf.placeholder(dtype=tf.float32, shape=[None])\n",
    "    \n",
    "\n",
    "    \n",
    "    episodes = tf.Variable(0.0, trainable=False, name='episodes')\n",
    "    episode_inc_op = episodes.assign_add(1)\n",
    "    \n",
    "    def __init__(self):\n",
    "        with tf.variable_scope('actor'):\n",
    "            self.actor_net_value = ANN.generate_actor_network(self,trainable = True, reuse = False)\n",
    "              \n",
    "        \n",
    "        with tf.variable_scope('slow_target_actor', reuse=False):\n",
    "            self.target_actor_net_value = tf.stop_gradient(ANN.generate_actor_network(self,trainable = False, reuse = False))\n",
    "\n",
    "        with tf.variable_scope('critic') as scope:\n",
    "            self.critic_net_value = ANN.generate_critic_network(self,trainable = True, reuse = False)\n",
    "            self.q_value_for_actor_net = ANN.generate_critic_network(self,trainable = True, reuse = True,mode=2)\n",
    "\n",
    "        \n",
    "        with tf.variable_scope('slow_target_critic', reuse=False):\n",
    "            self.target_critic_net_value = tf.stop_gradient(ANN.generate_critic_network(self,trainable = False, reuse = False,mode=3))\n",
    "        \n",
    "        \n",
    "        self.actor_net_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='actor')\n",
    "        self.target_actor_net_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='slow_target_actor')\n",
    "        self.critic_net_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='critic')\n",
    "        self.target_critic_net_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='slow_target_critic')\n",
    "\n",
    "        \n",
    "    def predict_graph(self):\n",
    "        return self.actor_net_value\n",
    "    def generate_actor_network(self,trainable, reuse):\n",
    "        layer1_out = tf.layers.conv2d(ANN.state_ph, filters=16, kernel_size=[8, 8],\n",
    "                                      strides=[4, 4], padding='same', activation=tf.nn.relu, data_format='channels_last', name='actor_layer1_out')\n",
    "        layer2_out = tf.layers.conv2d(layer1_out, filters=32, kernel_size=[4, 4],\n",
    "                                      strides=[2, 2], padding='same', activation=tf.nn.relu, data_format='channels_last', name='actor_layer2_out')\n",
    "        layer3_out = tf.layers.dense(tf.layers.flatten(layer2_out), 256, activation=tf.nn.relu, name='actor_layer3_out')\n",
    "        #output = tf.layers.dense(layer3_out, action_size, activation=None, name='output')\n",
    "        #hidden = tf.layers.dense(ANN.state_ph, h1_actor, activation = tf.nn.relu, trainable = trainable, name = 'dense', reuse = reuse)\n",
    "        #hidden_2 = tf.layers.dense(hidden, h2_actor, activation = tf.nn.relu, trainable = trainable, name = 'dense_1', reuse = reuse)\n",
    "        #hidden_3 = tf.layers.dense(hidden_2, h3_actor, activation = tf.nn.relu, trainable = trainable, name = 'dense_2', reuse = reuse)\n",
    "        actions_unscaled = tf.layers.dense(layer3_out, action_dim,activation=tf.nn.tanh, trainable = trainable, name = 'dense_3', reuse = reuse)\n",
    "        #actions = env.action_space.low + tf.nn.sigmoid(actions_unscaled)*(env.action_space.high - env.action_space.low) # bound the actions to the valid range\n",
    "        return actions_unscaled\n",
    "\n",
    "   \n",
    "    \n",
    "    def generate_critic_network(self,trainable, reuse,mode=1):\n",
    "        layer1_out = tf.layers.conv2d(ANN.state_ph, filters=16, kernel_size=[8, 8],\n",
    "                                      strides=[4, 4], padding='same', activation=tf.nn.relu, data_format='channels_last', name='critic_layer1_out',reuse = reuse)\n",
    "        layer2_out = tf.layers.conv2d(layer1_out, filters=32, kernel_size=[4, 4],\n",
    "                                      strides=[2, 2], padding='same', activation=tf.nn.relu, data_format='channels_last', name='critic_layer2_out',reuse = reuse)\n",
    "        layer2_flat=tf.layers.flatten(layer2_out)\n",
    "        if mode==1:\n",
    "            state_action = tf.concat([layer2_flat, ANN.action_ph], axis=1)\n",
    "        if mode==2:\n",
    "            state_action = tf.concat([layer2_flat,self.actor_net_value], axis=1)\n",
    "        if mode==3:\n",
    "            state_action = tf.concat([layer2_flat,self.target_actor_net_value], axis=1)\n",
    "        layer3_out = tf.layers.dense(state_action, 256, activation=tf.nn.relu, name='critic_layer3_out',reuse = reuse)\n",
    "        #hidden = tf.layers.dense(state_action, h1_critic, activation = tf.nn.relu, trainable = trainable, name = 'dense', reuse = reuse)\n",
    "        #hidden_2 = tf.layers.dense(hidden, h2_critic, activation = tf.nn.relu, trainable = trainable, name = 'dense_1', reuse = reuse)\n",
    "        #hidden_3 = tf.layers.dense(hidden_2, h3_critic, activation = tf.nn.relu, trainable = trainable, name = 'dense_2', reuse = reuse)\n",
    "        q_values = tf.layers.dense(layer3_out, 1, trainable = trainable, name = 'dense_3', reuse = reuse)\n",
    "        return q_values\n",
    "    def train_graph(self):\n",
    "        updated_q_values = tf.expand_dims(ANN.reward_ph, 1) + tf.expand_dims(ANN.is_not_terminal_ph, 1) * gamma * self.target_critic_net_value\n",
    "        td_errors = updated_q_values - self.critic_net_value\n",
    "\n",
    "\n",
    "        critic_loss = tf.reduce_mean(tf.square(td_errors))\n",
    "        for var in self.critic_net_vars:\n",
    "            if not 'bias' in var.name:\n",
    "                critic_loss += l2_reg_critic * 0.5 * tf.nn.l2_loss(var)\n",
    "\n",
    "\n",
    "        critic_train_op = tf.train.AdamOptimizer(lr_critic).minimize(critic_loss)\n",
    "        actor_loss = -1*tf.reduce_mean(self.q_value_for_actor_net)\n",
    "        for var in self.actor_net_vars:\n",
    "            if not 'bias' in var.name:\n",
    "                actor_loss += l2_reg_actor * 0.5 * tf.nn.l2_loss(var)\n",
    "\n",
    "        actor_train_op = tf.train.AdamOptimizer(lr_actor).minimize(actor_loss, var_list=self.actor_net_vars)\n",
    "        return actor_train_op,critic_train_op\n",
    "        \n",
    "    def update_wts_graph(self):\n",
    "            update_slow_target_ops = []\n",
    "            for i, target_actor_var in enumerate(self.target_actor_net_vars):\n",
    "                update_slow_target_actor_op = target_actor_var.assign(tau*self.actor_net_vars[i]+(1-tau)*target_actor_var)\n",
    "                update_slow_target_ops.append(update_slow_target_actor_op)\n",
    "\n",
    "            for i, slow_target_var in enumerate(self.target_critic_net_vars):\n",
    "                update_slow_target_critic_op = slow_target_var.assign(tau*self.critic_net_vars[i]+(1-tau)*slow_target_var)\n",
    "                update_slow_target_ops.append(update_slow_target_critic_op)\n",
    "\n",
    "            update_slow_targets_op = tf.group(*update_slow_target_ops, name='update_slow_targets')\n",
    "            return update_slow_targets_op \n",
    "    def load_weights(self,_actor_net_vars,_critic_net_vars):\n",
    "            update_wts_ops = []\n",
    "            for i, actor_var in enumerate(self.actor_net_vars):\n",
    "                update_actor_op = actor_var.assign(_actor_net_vars[i])\n",
    "                update_wts_ops.append(update_actor_op)\n",
    "\n",
    "            for i, critic_var in enumerate(self.critic_net_vars):\n",
    "                update_critic_op = critic_var.assign(_critic_net_vars[i])\n",
    "                update_wts_ops.append(update_critic_op)\n",
    "\n",
    "            update_wts_ops = tf.group(*update_wts_ops, name='update_wts_ops')\n",
    "            return update_wts_ops \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-c8893e5dc2b2>:46: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "WARNING:tensorflow:From <ipython-input-4-c8893e5dc2b2>:46: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "WARNING:tensorflow:From C:\\Users\\irfan\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_core\\python\\layers\\convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From C:\\Users\\irfan\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_core\\python\\layers\\convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-4-c8893e5dc2b2>:49: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-4-c8893e5dc2b2>:49: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-4-c8893e5dc2b2>:49: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From <ipython-input-4-c8893e5dc2b2>:49: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n"
     ]
    }
   ],
   "source": [
    "Model=ANN()\n",
    "actor_net_value=Model.predict_graph()\n",
    "actor_train_op,critic_train_op=Model.train_graph()\n",
    "update_wts_op=Model.update_wts_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# initialize session\n",
    "#sess = tf.Session()\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1143..1442 -> 299-tiles track\n",
      "Track generation: 1143..1442 -> 299-tiles track\n",
      "Episode  0, Reward: -83.221, Steps: 1000, noise:   0.200\n",
      "Track generation: 1087..1369 -> 282-tiles track\n",
      "Episode  0, Reward: -83.221, Steps: 1000, noise:   0.200\n",
      "Track generation: 1087..1369 -> 282-tiles track\n",
      "Episode  1, Reward: -85.765, Steps: 1000, noise:   0.198\n",
      "Track generation: 964..1212 -> 248-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1176..1474 -> 298-tiles track\n",
      "Episode  1, Reward: -85.765, Steps: 1000, noise:   0.198\n",
      "Track generation: 964..1212 -> 248-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1176..1474 -> 298-tiles track\n",
      "Episode  2, Reward: -86.532, Steps: 1000, noise:   0.196\n",
      "Track generation: 1283..1608 -> 325-tiles track\n",
      "Episode  2, Reward: -86.532, Steps: 1000, noise:   0.196\n",
      "Track generation: 1283..1608 -> 325-tiles track\n",
      "Episode  3, Reward: -87.654, Steps: 1000, noise:   0.194\n",
      "Track generation: 1217..1526 -> 309-tiles track\n",
      "Episode  3, Reward: -87.654, Steps: 1000, noise:   0.194\n",
      "Track generation: 1217..1526 -> 309-tiles track\n",
      "Episode  4, Reward: -87.013, Steps: 1000, noise:   0.192\n",
      "Track generation: 1096..1374 -> 278-tiles track\n",
      "Episode  4, Reward: -87.013, Steps: 1000, noise:   0.192\n",
      "Track generation: 1096..1374 -> 278-tiles track\n",
      "Episode  5, Reward: -85.560, Steps: 1000, noise:   0.190\n",
      "Track generation: 1198..1501 -> 303-tiles track\n",
      "Episode  5, Reward: -85.560, Steps: 1000, noise:   0.190\n",
      "Track generation: 1198..1501 -> 303-tiles track\n",
      "Episode  6, Reward: -86.755, Steps: 1000, noise:   0.188\n",
      "Track generation: 1159..1453 -> 294-tiles track\n",
      "Episode  6, Reward: -86.755, Steps: 1000, noise:   0.188\n",
      "Track generation: 1159..1453 -> 294-tiles track\n",
      "Episode  7, Reward: -86.348, Steps: 1000, noise:   0.186\n",
      "Track generation: 957..1205 -> 248-tiles track\n",
      "Episode  7, Reward: -86.348, Steps: 1000, noise:   0.186\n",
      "Track generation: 957..1205 -> 248-tiles track\n",
      "Episode  8, Reward: -83.806, Steps: 1000, noise:   0.185\n",
      "Track generation: 1181..1480 -> 299-tiles track\n",
      "Episode  8, Reward: -83.806, Steps: 1000, noise:   0.185\n",
      "Track generation: 1181..1480 -> 299-tiles track\n",
      "Episode  9, Reward: -86.577, Steps: 1000, noise:   0.183\n",
      "Track generation: 979..1234 -> 255-tiles track\n",
      "Episode  9, Reward: -86.577, Steps: 1000, noise:   0.183\n",
      "Track generation: 979..1234 -> 255-tiles track\n",
      "Episode 10, Reward: -84.252, Steps: 1000, noise:   0.181\n",
      "Track generation: 1320..1654 -> 334-tiles track\n",
      "Episode 10, Reward: -84.252, Steps: 1000, noise:   0.181\n",
      "Track generation: 1320..1654 -> 334-tiles track\n",
      "Episode 11, Reward: -87.988, Steps: 1000, noise:   0.179\n",
      "Track generation: 1067..1338 -> 271-tiles track\n",
      "Episode 11, Reward: -87.988, Steps: 1000, noise:   0.179\n",
      "Track generation: 1067..1338 -> 271-tiles track\n",
      "Episode 12, Reward: -85.185, Steps: 1000, noise:   0.177\n",
      "Track generation: 1067..1338 -> 271-tiles track\n",
      "Episode 12, Reward: -85.185, Steps: 1000, noise:   0.177\n",
      "Track generation: 1067..1338 -> 271-tiles track\n",
      "Episode 13, Reward: -85.185, Steps: 1000, noise:   0.176\n",
      "Track generation: 1207..1513 -> 306-tiles track\n",
      "Episode 13, Reward: -85.185, Steps: 1000, noise:   0.176\n",
      "Track generation: 1207..1513 -> 306-tiles track\n",
      "Episode 14, Reward: -86.885, Steps: 1000, noise:   0.174\n",
      "Track generation: 1106..1396 -> 290-tiles track\n",
      "Episode 14, Reward: -86.885, Steps: 1000, noise:   0.174\n",
      "Track generation: 1106..1396 -> 290-tiles track\n",
      "Episode 15, Reward: -86.159, Steps: 1000, noise:   0.172\n",
      "Track generation: 1296..1628 -> 332-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1047..1319 -> 272-tiles track\n",
      "Episode 15, Reward: -86.159, Steps: 1000, noise:   0.172\n",
      "Track generation: 1296..1628 -> 332-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1047..1319 -> 272-tiles track\n",
      "Episode 16, Reward: -85.240, Steps: 1000, noise:   0.170\n",
      "Track generation: 1245..1560 -> 315-tiles track\n",
      "Episode 16, Reward: -85.240, Steps: 1000, noise:   0.170\n",
      "Track generation: 1245..1560 -> 315-tiles track\n",
      "Episode 17, Reward: -87.261, Steps: 1000, noise:   0.169\n",
      "Track generation: 1232..1544 -> 312-tiles track\n",
      "Episode 17, Reward: -87.261, Steps: 1000, noise:   0.169\n",
      "Track generation: 1232..1544 -> 312-tiles track\n",
      "Episode 18, Reward: -87.138, Steps: 1000, noise:   0.167\n",
      "Track generation: 1120..1408 -> 288-tiles track\n",
      "Episode 18, Reward: -87.138, Steps: 1000, noise:   0.167\n",
      "Track generation: 1120..1408 -> 288-tiles track\n",
      "Episode 19, Reward: -86.063, Steps: 1000, noise:   0.165\n",
      "Track generation: 1225..1536 -> 311-tiles track\n",
      "Episode 19, Reward: -86.063, Steps: 1000, noise:   0.165\n",
      "Track generation: 1225..1536 -> 311-tiles track\n",
      "Episode 20, Reward: -87.097, Steps: 1000, noise:   0.164\n",
      "Track generation: 1077..1357 -> 280-tiles track\n",
      "Episode 20, Reward: -87.097, Steps: 1000, noise:   0.164\n",
      "Track generation: 1077..1357 -> 280-tiles track\n"
     ]
    }
   ],
   "source": [
    "#####################################################################################################\n",
    "## Training\n",
    "\n",
    "total_steps = 0\n",
    "log_rewards=[]\n",
    "for ep in range(num_episodes):\n",
    "\n",
    "    total_reward = 0\n",
    "    steps_in_ep = 0\n",
    "\n",
    "    \n",
    "    noise_process = np.zeros(action_dim)\n",
    "    noise_scale = (initial_noise_scale * noise_decay**ep) * (env.action_space.high - env.action_space.low)\n",
    "\n",
    "    \n",
    "    observation = env.reset()\n",
    "    for t in range(max_steps_ep):\n",
    "        obs=observation/255.0\n",
    "        action_for_state, = sess.run(actor_net_value,\n",
    "            feed_dict = {Model.state_ph:obs[None] })\n",
    "\n",
    "        \n",
    "        noise_process = exploration_theta*(exploration_mu - noise_process) + exploration_sigma*np.random.randn(action_dim)\n",
    "        action_for_state += noise_scale*noise_process\n",
    "\n",
    "        \n",
    "        \n",
    "        if t<=1000:\n",
    "            #action_for_state[0]=0.0\n",
    "            action_for_state[1]=0.5\n",
    "            action_for_state[2]=0.0\n",
    "        \n",
    "        next_observation, reward, done, _info = env.step(action_for_state)\n",
    "        total_reward += reward\n",
    "\n",
    "        add_to_memory((obs , action_for_state, reward, next_observation, \n",
    "            0.0 if done else 1.0))\n",
    "        \n",
    "        if total_steps%train_every == 0 and len(replay_memory) >= minibatch_size:\n",
    "\n",
    "           \n",
    "            minibatch = sample_from_memory(minibatch_size)\n",
    "\n",
    "            _, _ = sess.run([critic_train_op, actor_train_op], \n",
    "                feed_dict = {\n",
    "                    Model.state_ph: np.asarray([elem[0] for elem in minibatch]),\n",
    "                    Model.action_ph: np.asarray([elem[1] for elem in minibatch]),\n",
    "                    Model.reward_ph: np.asarray([elem[2] for elem in minibatch]),\n",
    "                    Model.next_state_ph: np.asarray([elem[3] for elem in minibatch]),\n",
    "                    Model.is_not_terminal_ph: np.asarray([elem[4] for elem in minibatch]),\n",
    "                    })\n",
    "\n",
    "\n",
    "            _ = sess.run(update_wts_op)\n",
    "\n",
    "        observation = next_observation\n",
    "        total_steps += 1\n",
    "        steps_in_ep += 1\n",
    "        \n",
    "        if done: \n",
    "            \n",
    "            _ = sess.run(Model.episode_inc_op)\n",
    "            break\n",
    "    log_rewards.append([ep,total_reward])\n",
    "    print('Episode %2i, Reward: %7.3f, Steps: %i, noise: %7.3f'%(ep,total_reward,steps_in_ep, noise_scale[0]))\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c conda-forge tqdm\n",
    "#conda install -c conda-forge ipywidgets\n",
    "#conda install -c conda-forge nodejs#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tqdm.write('Episode %2i, Reward: %7.3f, Steps: %i, noise: %7.3f'%(ep,total_reward,steps_in_ep, noise_scale[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_weights(actor_net_var,critic_net_vars):\n",
    "            import pickle\n",
    "            f=open('net_wts','wb')\n",
    "            pickle.dump([actor_wts,critic_wts],f)\n",
    "            f.close() \n",
    "def load_weights(Model,sess):\n",
    "            import pickle\n",
    "            f=open('net_wts','rb')\n",
    "            [actor_wts,critic_wts]=pickle.load(f)\n",
    "            f.close()\n",
    "            sess.run(Model.load_weights(actor_wts,critic_wts))\n",
    "            return actor_wts,critic_wts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save=True\n",
    "if save==True:\n",
    "    actor_wts=sess.run(Model.actor_net_vars)\n",
    "    critic_wts=sess.run(Model.critic_net_vars)\n",
    "    save_weights(actor_wts,critic_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load=False\n",
    "if load==True:\n",
    "    load_weights(Model,sess)\n",
    "    a_wts=sess.run(Model.actor_net_vars)\n",
    "    c_wts=sess.run(Model.critic_net_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import cv2\n",
    "env=gym.make(env_to_use)\n",
    "env.seed(0)\n",
    "obs=env.reset()\n",
    "\n",
    "#os.mkdir(env_to_use+'Test')\n",
    "#os.mkdir(env_to_use+'Test/img/')\n",
    "for i in range(1000):\n",
    "    \n",
    "    img=obs/255.0\n",
    "    action_for_state, = sess.run(actor_net_value, \n",
    "                    feed_dict = {Model.state_ph: img[None]})\n",
    "    \n",
    "    if i<=1000:\n",
    "            #action_for_state[0]=0.0 \n",
    "            action_for_state[1]=0.5\n",
    "            action_for_state[2]=0.0\n",
    "    \n",
    "    obs,rew,done,info=env.step(action_for_state)\n",
    "    print(action_for_state,rew)\n",
    "    img=env.render(mode='rgb_array')\n",
    "    #time.sleep(0.1)\n",
    "    #cv2.imwrite(env_to_use+'Test/img/'+str(i)+'.jpg',img)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "rws=np.array(log_rewards)\n",
    "fig=plt.figure(figsize=(10,10))\n",
    "plt.plot(rws[:,0],rws[:,1])\n",
    "plt.title('epoch vs sum of reward')\n",
    "plt.savefig(env_to_use+'Test/'+'rewards.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame(rws[:,1],columns=['sum_of_rewards'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(env_to_use+'Test/rewards.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
